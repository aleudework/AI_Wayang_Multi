from openai import OpenAI
from ai_wayang_multi.config.settings import SPECIFIER_AGENT_CONFIG
from ai_wayang_multi.llm.prompt_loader import PromptLoader


class Specifier:
    """
    Agent to rewrite user request in clearly English with relevant specification.

    """

    def __init__(
        self,
        model: str | None = None,
        reasoning: str | None = None,
        system_prompt: str | None = None,
    ):
        self.client = OpenAI()
        self.model = model or SPECIFIER_AGENT_CONFIG.get("model")
        self.reasoning = reasoning or SPECIFIER_AGENT_CONFIG.get("reason_effort")
        self.system_prompt = (
            system_prompt or PromptLoader().load_specifier_system_prompt()
        )
        self.chat = []

    def set_model_and_reasoning(self, model: str, reasoning: str) -> None:
        """
        Sets objects model and reasoning if any.
        Mostly for testing

        Args:
            model (str): GPT-model
            reasoning (str): Reasoning level if any

        """
        self.model = model
        self.reasoning = reasoning

    def start(self) -> None:
        """
        Clears chat history without resetting agent

        """
        self.chat = [{"role": "system", "content": self.system_prompt}]

    def generate(self, prompt: str):
        """
        Refine user query and select relevant data sources to generate the plan

        Args:
            prompt (str): A query in natural language

        Returns:
            WayangPlanSpecification: Refined user query and selected data sources.

        """

        # Append user prompt to chat
        self.chat.append({"role": "user", "content": prompt})

        # Defines params and structured format for the model
        params = {"model": self.model, "input": self.chat}

        # Set effort if reasoning model
        effort = self.reasoning

        if effort:
            params["reasoning"] = {"effort": effort}

        # Generate response
        response = self.client.responses.parse(**params)

        # Return response
        return {"raw": response, "refined_query": response.output_text}
